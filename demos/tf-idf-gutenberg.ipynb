{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf demo on the books in the Gutenberg corpus in NLTK\n",
    "\n",
    "Let's compute tf-idf scores for words occurring in the different books in the NLTK version of the Gutenberg corpus.\n",
    "\n",
    "First we take a look at what books there are and how many words each of them contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "booknames = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "for name in booknames:\n",
    "    print(\"{:s} contains {:d} word tokens.\".format(name, len(nltk.corpus.gutenberg.words(name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how many word tokens and types there are in all books combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntot_tokens = sum(len(nltk.corpus.gutenberg.words(name)) for name in booknames)\n",
    "print(\"Total number of word tokens:\", ntot_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we handle all word types or do we need to drop some?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntot_types = len(set(w.lower() for bookname in booknames for w in nltk.corpus.gutenberg.words(bookname)))\n",
    "print(\"Total number of word types:\", ntot_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number of word types is not too high, so we can keep them all in our vocabulary.\n",
    "\n",
    "Let's look at which words are the most common in the corpus as a whole (**term frequencies** in the entire corpus). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(w.lower() for bookname in booknames for w in nltk.corpus.gutenberg.words(bookname))\n",
    "\n",
    "print(\"The 100 most common word types in the Gutenberg corpus are:\")\n",
    "for w, f in fdist.most_common(100):\n",
    "    print(w, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about **document frequencies**? For every word type, we count the number of documents (= books) it occurs in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "df = defaultdict(int)\n",
    "\n",
    "for name in booknames:\n",
    "    for word in set(w.lower() for w in nltk.corpus.gutenberg.words(name)):\n",
    "        df[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sort the words by their document frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted_words = sorted(df.keys(), key=lambda w: df[w], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some words that occur in _all_ the books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of words that occur in all books:\")\n",
    "for w in df_sorted_words[0:10]:\n",
    "    print(w, df[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some words that only occur in one single book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of words that occur in only one book:\")\n",
    "for w in df_sorted_words[-10:]:\n",
    "    print(w, df[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as some words that occur in a few books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of words that occur in some, but not all books:\")\n",
    "for w in df_sorted_words[5000:5010]:\n",
    "    print(w, df[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the document frequencies we compute the corresponding **inverse document frequency** (idf) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "ndocs = len(booknames)\n",
    "idf = defaultdict(float)\n",
    "\n",
    "for w in df.keys():\n",
    "    idf[w] = math.log(ndocs / df[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the idf values look like for words that occur in all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show the IDF values of some words that occur in all books:\")\n",
    "for w in df_sorted_words[0:10]:\n",
    "    print(w, idf[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or words that occur in one document only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show the IDF values of some words that occur in only one book:\")\n",
    "for w in df_sorted_words[-10:]:\n",
    "    print(w, idf[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or somewhere in between?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show the IDF values of some words that occur in some, but not all books:\")\n",
    "for w in df_sorted_words[5000:5010]:\n",
    "    print(w, idf[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the calculation of the tf-idf scores we also need the *document-specific term frequencies*. Let's look at the 20 most frequent words in each of the documents. Do we see any big differences between the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in booknames:\n",
    "    fdist = nltk.FreqDist(w.lower() for w in nltk.corpus.gutenberg.words(name))\n",
    "    print(\"Most frequent words in {:s}:\".format(name))\n",
    "    for w, f in fdist.most_common(20):\n",
    "        print(w, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look, when we compute **tf-idf** scores for the words in the documents and compare? We display the 20 top scoring words for each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each document (book) ...\n",
    "for bookname in booknames:\n",
    "    tf = {}\n",
    "    tfidf = {}\n",
    "    fdist = nltk.FreqDist(w.lower() for w in nltk.corpus.gutenberg.words(bookname))\n",
    "\n",
    "    # For each word in the document ...\n",
    "    for w, f in fdist.most_common():\n",
    "        # Compute the term frequency:\n",
    "        tf[w] = 1 + math.log10(f)\n",
    "        # ... as well as the tf-idf score:\n",
    "        tfidf[w] = tf[w] * idf[w]\n",
    "    \n",
    "    # Sort the words by tf-idf\n",
    "    tfidf_sorted_words = sorted(tfidf.keys(), key=lambda w: tfidf[w], reverse=True)\n",
    "    \n",
    "    # Show the highest scoring words in this document\n",
    "    print(\"Highest tf-idf scoring words in {:s}:\".format(bookname))\n",
    "    for w in tfidf_sorted_words[0:20]:\n",
    "        print(\"{:s}: {:.3f} (tf: {:.3f}, idf: {:.3f})\".format(w, tfidf[w], tf[w], idf[w]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun the the tf-idf calculations, but instead of showing the highest scoring words in each document, we will show the scores of some (arbitrarily) selected words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_words = [ \"god\", \"jesus\", \"rich\", \"handsome\", \"romantic\", \"secret\",\n",
    "                   \"honey\", \"bear\", \"ahab\", \"white\", \"whale\", \"alice\",\n",
    "                   \"wonderland\", \"king\", \"queen\", \"lazy\", \"sin\", \"paradise\" ]\n",
    "\n",
    "for bookname in booknames:\n",
    "    tf = {}\n",
    "    tfidf = {}\n",
    "    fdist = nltk.FreqDist(w.lower() for w in nltk.corpus.gutenberg.words(bookname))\n",
    "    for w, f in fdist.most_common():\n",
    "        tf[w] = 1 + math.log10(f)\n",
    "        tfidf[w] = tf[w] * idf[w]\n",
    "\n",
    "    print(\"Tf-idf scores of some selected words in {:s}:\".format(bookname))\n",
    "    for w in selected_words:\n",
    "        if w in tfidf.keys():\n",
    "            print(\"{:s}: {:.3f} (tf: {:.3f}, idf: {:.3f})\".format(w, tfidf[w], tf[w], idf[w]))\n",
    "        else:\n",
    "            print(\"{:s}: missing\".format(w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense?\n",
    "\n",
    "How come the tf-idf score is zero for some words? What might be the consequences of this? Are there any weaknesses in this approach? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
